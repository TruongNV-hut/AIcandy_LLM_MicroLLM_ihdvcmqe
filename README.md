# Language Model Training and Testing using Transformers

<p align="justify">
This project provides an in-depth exploration of building, training, and evaluating large language models (LLMs) based on Transformer architectures. It is designed to serve as an educational resource for understanding the fundamental components and methodologies behind creating state-of-the-art LLMs.

</p>

## Goals

- To provide a hands-on approach to understanding the inner workings of LLMs.
- To explore best practices for training and optimizing Transformer models.
- To offer a platform for experimenting with advanced NLP tasks.

## ❤️❤️❤️

```bash
If you find this project useful, please give it a star to show your support and help others discover it!
```

## Getting Started

### Clone the Repository

To get started with this project, clone the repository using the following command:

```bash
git clone https://github.com/TruongNV-hut/AIcandy_LLM_MicroLLM_ihdvcmqe.git
```

### Install Dependencies
Before running the scripts, you need to install the required libraries. You can do this using pip:

```bash
pip install -r requirements.txt
```

### Training the Model

To train the model, use the following command:

```bash
python AIcandy_LLM_Transformer_hkosbvui.py --mode train
```

### Evaluate the Model

To evaluate the model, use the following command:

```bash
python AIcandy_LLM_Transformer_hkosbvui.py --mode test --input <some input text>
```

### More Information

To learn more about this project, [see here](https://aicandy.vn/huong-dan-tao-chatgpt-xay-dung-ai-chatbot-voi-transformer).

To learn more about knowledge and real-world projects on Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL), visit the website [aicandy.vn](https://aicandy.vn/).

❤️❤️❤️




